{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments, AutoModelWithLMHead, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load poetries text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "Totale poesie processate: 766\n",
      "Lista autori:\n",
      "CAIO VALERIO CATULLO\n",
      "GIUSEPPE UNGARETTI\n",
      "JACK KEROUAC\n",
      "CARLO BETOCCHI\n",
      "ALESSANDRO MANZONI\n",
      "SALVATORE QUASIMODO\n",
      "JAMES JOYCE\n",
      "FRANCESCO D ASSISI\n",
      "PABLO NERUDA\n",
      "SAFFO\n",
      "WILLIAM BUTLER YEATS\n",
      "ARRIGO BOITO\n",
      "EDGAR ALLAN POE\n",
      "GIACOMO LEOPARDI\n",
      "JORGE LUIS BORGES\n",
      "NICCOLO UGO FOSCOLO\n",
      "OSCAR WILDE\n",
      "RUDYARD KIPLING\n",
      "VICTOR HUGO\n",
      "UMBERTO SABA\n",
      "ANNA ACHMATOVA\n",
      "VOLTAIRE\n",
      "ITALO CALVINO\n",
      "WILLIAM WORDSWORTH\n",
      "DANTE ALIGHIERI\n",
      "MICHELANGELO BUONARROTI\n",
      "ANNA FRANK\n",
      "EDUARDO DE FILIPPO\n",
      "ARTHUR RIMBAUD\n",
      "GIORGIO CAPRONI\n",
      "DINO BUZZATI\n",
      "MARCEL PROUST\n",
      "FRIEDRICH SCHILLER\n",
      "CHARLES BAUDELAIRE\n",
      "ALDA MERINI\n",
      "EMILY DICKINSON\n",
      "EDMONDO DE AMICIS\n",
      "CESARE PAVESE\n",
      "SAN PAOLO\n",
      "PRIMO LEVI\n",
      "GIOSUE CARDUCCI\n",
      "JOHN KEATS\n",
      "PIER PAOLO PASOLINI\n",
      "LUIGI PIRANDELLO\n",
      "WILLIAM BLAKE\n",
      "GUILLAUME APOLLINAIRE\n",
      "ADA NEGRI\n",
      "PAULO COELHO\n",
      "ROBERT FROST\n",
      "GIUSEPPE PARINI\n",
      "JOHN DONNE\n",
      "WILLIAM SHAKESPEARE\n",
      "GABRIELE D ANNUNZIO\n",
      "ALDO PALAZZESCHI\n",
      "EZRA POUND\n",
      "GUIDO CAVALCANTI\n",
      "THOMAS STEARNS ELIOT\n",
      "FRANCESCO PETRARCA\n",
      "CORRADO GOVONI\n",
      "CHARLES BUKOWSKI\n",
      "LEWIS CARROLL\n",
      "EUGENIO MONTALE\n",
      "GIOVANNI PASCOLI\n",
      "-------------------------\n",
      "Train dataset length: 651\n",
      "Test dataset length: 115\n"
     ]
    }
   ],
   "source": [
    "#%% open poetries file and keep only authors with more than 5 poetries\n",
    "filename = os.path.join('data_collection', 'poestries_dict.pkl')\n",
    "with open(filename, 'rb') as f:\n",
    "    poetries_dict = pickle.load(f)\n",
    "\n",
    "# define authors to keep\n",
    "authors_to_keep = [\n",
    "    \"JOHN KEATS\",\n",
    "    \"JOHN DONNE\",\n",
    "    \"LUIGI PIRANDELLO\",\n",
    "    \"ALDO PALAZZESCHI\",\n",
    "    \"ANNA ACHMATOVA\",\n",
    "    \"GIACOMO LEOPARDI\",\n",
    "    \"GIUSEPPE PARINI\",\n",
    "    \"SAFFO\",\n",
    "    \"EDMONDO DE AMICIS\",\n",
    "    \"FRANCESCO PETRARCA\",\n",
    "    \"WILLIAM WORDSWORTH\",\n",
    "    \"ROBERT FROST\",\n",
    "    \"DINO BUZZATI\",\n",
    "    \"MARCEL PROUST\",\n",
    "    \"VOLTAIRE\",\n",
    "    \"GUILLAUME APOLLINAIRE\",\n",
    "    \"EZRA POUND\",\n",
    "    \"JAMES JOYCE\",\n",
    "    \"GIUSEPPE UNGARETTI\",\n",
    "    \"SALVATORE QUASIMODO\",\n",
    "    \"WILLIAM BLAKE\",\n",
    "    \"JORGE LUIS BORGES\",\n",
    "    \"PRIMO LEVI\",\n",
    "    \"GABRIELE D ANNUNZIO\",\n",
    "    \"PAULO COELHO\",\n",
    "    \"EMILY DICKINSON\",\n",
    "    \"CHARLES BUKOWSKI\",\n",
    "    \"UMBERTO SABA\",\n",
    "    \"SAN PAOLO\",\n",
    "    \"FRIEDRICH SCHILLER\",\n",
    "    \"ARRIGO BOITO\",\n",
    "    \"WILLIAM SHAKESPEARE\",\n",
    "    \"CORRADO GOVONI\",\n",
    "    \"WILLIAM BUTLER YEATS\",\n",
    "    \"EDGAR ALLAN POE\",\n",
    "    \"VICTOR HUGO\",\n",
    "    \"ITALO CALVINO\",\n",
    "    \"ADA NEGRI\",\n",
    "    \"CARLO BETOCCHI\",\n",
    "    \"CESARE PAVESE\",\n",
    "    \"GIOVANNI PASCOLI\",\n",
    "    \"CHARLES BAUDELAIRE\",\n",
    "    \"JACK KEROUAC\",\n",
    "    \"GUIDO CAVALCANTI\",\n",
    "    \"CAIO VALERIO CATULLO\",\n",
    "    \"FRANCESCO D ASSISI\",\n",
    "    \"EDUARDO DE FILIPPO\",\n",
    "    \"THOMAS STEARNS ELIOT\",\n",
    "    \"NICCOLO UGO FOSCOLO\",\n",
    "    \"OSCAR WILDE\",\n",
    "    \"EUGENIO MONTALE\",\n",
    "    \"DANTE ALIGHIERI\",\n",
    "    \"PABLO NERUDA\",\n",
    "    \"ARTHUR RIMBAUD\",\n",
    "    \"ALESSANDRO MANZONI\",\n",
    "    \"RUDYARD KIPLING\",\n",
    "    \"ANNA FRANK\",\n",
    "    \"ALDA MERINI\",\n",
    "    \"PIER PAOLO PASOLINI\",\n",
    "    \"LEWIS CARROLL\",\n",
    "    \"GIOSUE CARDUCCI\",\n",
    "    \"GIORGIO CAPRONI\",\n",
    "    \"MICHELANGELO BUONARROTI\"\n",
    "]\n",
    "\n",
    "poetries = []\n",
    "author_list = []\n",
    "for key in poetries_dict:\n",
    "    author = key.replace('-', ' ').upper()\n",
    "    if author in authors_to_keep:\n",
    "        for p in poetries_dict[key]:\n",
    "            poetries.append(p)\n",
    "            author_list.append(author)\n",
    "\n",
    "print('-------------------------')\n",
    "print('Totale poesie processate: {}'.format(len(poetries)))\n",
    "print('Lista autori:')\n",
    "for author in set(author_list):\n",
    "    print(author)\n",
    "print('-------------------------')\n",
    "\n",
    "#%% prepare dataset\n",
    "\n",
    "table = str.maketrans('', '', '!\"#$%&\\'()*+-/:;<=>?@[\\\\]^_`{|}~¬ª‚Äî‚Ä¶¬π‚Äù¬®¬´‚Äò‚Äú¬¨ÀÜ')\n",
    "for i in range(len(poetries)):\n",
    "    poetries[i] = poetries[i].lower()\n",
    "    poetries[i] = poetries[i].replace(\"\\r\", \"\")\n",
    "    poetries[i] = poetries[i].replace(\"\\n\", \" \\n \")\n",
    "    poetries[i] = poetries[i].replace(\"  \", \" \")\n",
    "    poetries[i] = poetries[i].replace(\"√¢‚Ç¨‚Ñ¢\", \"'\")\n",
    "    poetries[i] = poetries[i].replace(\"‚Äô\", \" \")\n",
    "    # poetries[i] = poetries[i].replace(\",\", \" , \")\n",
    "    poetries[i] = poetries[i].replace(\".\", \". \")\n",
    "    poetries[i] = poetries[i].replace(\"  \", \" \")\n",
    "    poetries[i] = poetries[i].replace(\"√∫\", \"√π\")\n",
    "    poetries[i] = poetries[i].replace(\"√£\", \"a\")\n",
    "    poetries[i] = poetries[i].replace(\"√¢\", \"a\")\n",
    "    poetries[i] = poetries[i].replace(\"√≠\", \"√¨\")\n",
    "    poetries[i] = poetries[i].replace(\"√¥\", \"o\")\n",
    "    poetries[i] = poetries[i].replace(\"a¬©\", \"√®\")\n",
    "    poetries[i] = poetries[i].replace(\"√Ø\", \"i\")\n",
    "    poetries[i] = poetries[i].translate(table)\n",
    "poetries = np.array(poetries)\n",
    "authors = np.array(author_list)\n",
    "\n",
    "\n",
    "def build_text_files(poetries, dest_path):\n",
    "    data = ''\n",
    "    for text in poetries:\n",
    "        data = data + \"<|endoftext|>\" + text\n",
    "    with open(dest_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(data)\n",
    "\n",
    "train, test = train_test_split(poetries, test_size=0.15)\n",
    "\n",
    "build_text_files(train, os.path.join('data_collection', 'train_dataset.txt'))\n",
    "build_text_files(test, os.path.join('data_collection', 'test_dataset.txt'))\n",
    "\n",
    "print(\"Train dataset length: \" + str(len(train)))\n",
    "print(\"Test dataset length: \" + str(len(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model and tokenizer from pre-trained small italian GPT2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\apazzaglia00\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:588: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "c:\\Users\\apazzaglia00\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:54: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelWithLMHead.from_pretrained(\"GroNLP/gpt2-small-italian\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"GroNLP/gpt2-small-italian\")\n",
    "\n",
    "train_path = os.path.join('data_collection', 'train_dataset.txt')\n",
    "test_path = os.path.join('data_collection', 'test_dataset.txt')\n",
    "\n",
    "# train_encoded = tokenizer.encode('\\n'.join([text for text in train]))\n",
    "# train_decoded = tokenizer.decode(train_encoded)\n",
    "# with open(os.path.join('data_collection', 'train_dataset_encoded.txt'), 'w', encoding='utf-8') as f:\n",
    "#     f.write(train_decoded)\n",
    "\n",
    "def load_dataset(train_path,test_path,tokenizer):\n",
    "    train_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=train_path,\n",
    "          block_size=128)\n",
    "\n",
    "    test_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=test_path,\n",
    "          block_size=128)\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False,\n",
    "    )\n",
    "    return train_dataset, test_dataset, data_collator\n",
    "\n",
    "train_dataset, test_dataset, data_collator = load_dataset(train_path, test_path, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining fine-tuning parameters and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 1750\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 165\n",
      " 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 85/165 [50:21<54:18, 40.73s/it]  "
     ]
    }
   ],
   "source": [
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(\"models\", \"gpt2-poetries\"), #The output directory\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=3, # number of training epochs\n",
    "    per_device_train_batch_size=32, # batch size for training\n",
    "    per_device_eval_batch_size=64,  # batch size for evaluation\n",
    "    eval_steps=200, # Number of update steps between two evaluations.\n",
    "    save_steps=200, # after # steps model is saved\n",
    "    warmup_steps=200, # number of warmup steps for learning rate scheduler\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model output with custom prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-generation\", model=os.path.join(\"models\", \"gpt2-poetries\"), tokenizer=tokenizer)\n",
    "result = pipe(\"il tuo sorriso √® come\")[0]['generated_text']\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "923bb44e77603537bf5129924008506bd2cdd0af89ebee59c5e5508f0654874d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
